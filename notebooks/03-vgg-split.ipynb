{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "from fastai.fautils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data_path = os.path.expanduser('~/data/state-farm/')\n",
    "samp_data_path = os.path.expanduser('~/data/sample-state-farm/')\n",
    "data_path = full_data_path\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we experiment with splitting VGG into to convolutional and fully connected layers to accelerate the fine-tuning process.  When fine-tuning, we generally don't alter the convolutional layers.  They've been learned from very large datasets and tend to be tuned to the distribution of image content (to some degree more or less depending on the distribution of data and labels -- ala the Imagenet tendency towards dog faces).\n",
    "\n",
    "The reason for this approach is that most of the computational work is in the convolutional layers.  Since they tend not to change in most experiments (the computational cost of training them anew to convergence is too high), we can precompute them.  Then we can construct a second network that takes the output of the convolution-only network as input and can iterate much more rapidly on that.\n",
    "\n",
    "We're going to:\n",
    "\n",
    "- construct a conventional VGG model\n",
    "- split it at the division between the convolutional layers and the FC layers\n",
    "- we'll take our entire dataset and run it through the convolutional layers, and store the result\n",
    "- we'll then construct a secondary model that takes the convolutional output for input, and outputs the desired prediction\n",
    "- we'll then train that network on the stored convolutional output\n",
    "\n",
    "This way we can reduce a 10+ minute training per epoch to 22 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20315 images belonging to 10 classes.\n",
      "Found 2109 images belonging to 10 classes.\n",
      "Found 20315 images belonging to 10 classes.\n",
      "Found 20315 images belonging to 10 classes.\n",
      "Found 2109 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    rotation_range=15,\n",
    "    channel_shift_range=30\n",
    ")\n",
    "\n",
    "t_batches = get_batches(data_path + 'train', batch_size=batch_size, shuffle=False)\n",
    "v_batches = get_batches(data_path + 'valid', batch_size=2*batch_size, shuffle=False)\n",
    "a_batches = get_batches(data_path + 'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "(\n",
    "    val_classes, trn_classes, \n",
    "    val_labels, trn_labels, \n",
    "    val_filenames, filenames,\n",
    "    test_filename\n",
    ") = get_classes(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct initial VGG network, then split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import vgg162\n",
    "from keras.applications import VGG16\n",
    "\n",
    "# vgg = vgg_ft(10)\n",
    "# model = vgg.model\n",
    "\n",
    "model = VGG16(pooling='max')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def finetune(model, n):\n",
    "#     model.layers.pop()\n",
    "#     for layer in model.layers: \n",
    "#         layer.trainable=False\n",
    "#     model.layers.add(keras.layers.Dense(num, activation='softmax'))\n",
    "#     self.compile()\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = finetune(model, 10)\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 25.\n",
    "decay_rate = lr / epochs\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=1e-3, decay=decay_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h = model.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=25, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x7fd256e30c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = model.layers\n",
    "\n",
    "last_conv_idx = [\n",
    "    index \n",
    "    for index, layer in enumerate(layers)\n",
    "    if type(layer) is Conv2D\n",
    "][-1]\n",
    "\n",
    "print(last_conv_idx)\n",
    "layers[last_conv_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layers = layers[:last_conv_idx+1]\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_layers = layers[last_conv_idx+1:]\n",
    "fc_model = Sequential(fc_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precompute, store reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features = conv_model.predict_generator(t_batches, t_batches.samples)\n",
    "v_features = conv_model.predict_generator(v_batches, v_batches.samples)\n",
    "\n",
    "save_array(data_path + 'train_convlayer_features.bc', t_features)\n",
    "save_array(data_path + 'valid_convlayer_features.bc', v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features = load_array(data_path + 'train_convlayer_features.bc')\n",
    "v_features = load_array(data_path + 'valid_convlayer_features.bc')\n",
    "\n",
    "t_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create second network and train\n",
    "\n",
    "**NOTE:** I had hoped to just use the 2nd half of the split network above directly, but that didn't work.  I needed to recreate it with the same configuration as I did here.  Or I could have chose another configuration entirely.\n",
    "\n",
    "Regardless, I think you need to manually recreate the new network, opposed to borrowing the previously-split one.  That's something to be validated.\n",
    "\n",
    "The most important part is the first level.  I'm not sure why the tutorial said to include the max pooling layer here, opposed to leave it in the convolutional precomputation.  But the `input_shape` is important to match to the dimensions shown in `t_features.shape` above.  Except the first dimension, that's the numeber of examples.  We exclude that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "epochs = 25.\n",
    "decay_rate = lr / epochs\n",
    "\n",
    "model = Sequential([\n",
    "    MaxPooling2D((2, 2), strides=(2, 2), input_shape=(512, 14, 14)),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=lr, decay=decay_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20424 samples, validate on 2000 samples\n",
      "Epoch 1/25\n",
      "20424/20424 [==============================] - 22s - loss: 7.5257 - acc: 0.4363 - val_loss: 1.1177 - val_acc: 0.9025\n",
      "Epoch 2/25\n",
      "20424/20424 [==============================] - 22s - loss: 1.8291 - acc: 0.8290 - val_loss: 0.2732 - val_acc: 0.9705\n",
      "Epoch 3/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.8057 - acc: 0.9198 - val_loss: 0.1633 - val_acc: 0.9815\n",
      "Epoch 4/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.4859 - acc: 0.9471 - val_loss: 0.1060 - val_acc: 0.9905\n",
      "Epoch 5/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.3180 - acc: 0.9634 - val_loss: 0.0671 - val_acc: 0.9910\n",
      "Epoch 6/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.2378 - acc: 0.9720 - val_loss: 0.0513 - val_acc: 0.9935\n",
      "Epoch 7/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.1759 - acc: 0.9778 - val_loss: 0.0481 - val_acc: 0.9960\n",
      "Epoch 8/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.1478 - acc: 0.9820 - val_loss: 0.0465 - val_acc: 0.9965\n",
      "Epoch 9/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.1151 - acc: 0.9854 - val_loss: 0.0430 - val_acc: 0.9955\n",
      "Epoch 10/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.1109 - acc: 0.9864 - val_loss: 0.0452 - val_acc: 0.9955\n",
      "Epoch 11/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0919 - acc: 0.9886 - val_loss: 0.0438 - val_acc: 0.9965\n",
      "Epoch 12/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0920 - acc: 0.9882 - val_loss: 0.0394 - val_acc: 0.9955\n",
      "Epoch 13/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0832 - acc: 0.9899 - val_loss: 0.0338 - val_acc: 0.9970\n",
      "Epoch 14/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0752 - acc: 0.9906 - val_loss: 0.0354 - val_acc: 0.9970\n",
      "Epoch 15/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0668 - acc: 0.9917 - val_loss: 0.0436 - val_acc: 0.9965\n",
      "Epoch 16/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0592 - acc: 0.9920 - val_loss: 0.0482 - val_acc: 0.9955\n",
      "Epoch 17/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0689 - acc: 0.9916 - val_loss: 0.0444 - val_acc: 0.9955\n",
      "Epoch 18/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0641 - acc: 0.9916 - val_loss: 0.0166 - val_acc: 0.9985\n",
      "Epoch 19/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0565 - acc: 0.9928 - val_loss: 0.0133 - val_acc: 0.9985\n",
      "Epoch 20/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0529 - acc: 0.9936 - val_loss: 0.0247 - val_acc: 0.9975\n",
      "Epoch 21/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0515 - acc: 0.9940 - val_loss: 0.0108 - val_acc: 0.9990\n",
      "Epoch 22/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0555 - acc: 0.9937 - val_loss: 0.0378 - val_acc: 0.9960\n",
      "Epoch 23/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0402 - acc: 0.9945 - val_loss: 0.0228 - val_acc: 0.9975\n",
      "Epoch 24/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0418 - acc: 0.9951 - val_loss: 0.0118 - val_acc: 0.9985\n",
      "Epoch 25/25\n",
      "20424/20424 [==============================] - 22s - loss: 0.0519 - acc: 0.9936 - val_loss: 0.0183 - val_acc: 0.9980\n"
     ]
    }
   ],
   "source": [
    "h = model.fit(\n",
    "    t_features, \n",
    "    trn_labels, \n",
    "    nb_epoch=25,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(v_features, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe29d98b7d0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe29d98b390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h.history['acc'])\n",
    "plt.plot(h.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
