{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: Tesla K80 (0000:00:1E.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import fastai\n",
    "import fastai.utils\n",
    "from fastai.fautils import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data_path = os.path.expanduser('~/data/state-farm/')\n",
    "samp_data_path = os.path.expanduser('~/data/sample-state-farm/')\n",
    "data_path = samp_data_path\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview\n",
    "\n",
    "This notebook shows a lot of exploratory work with the State Farm kaggle challenge.  The biggest takeaways are:\n",
    "\n",
    "1. Find the smallest sample size that produces consistent results\n",
    "1. Start with very small models and quickly work up in complexity, till you're overfitting\n",
    "1. Selecting the initial training rate, and adjusting it through training is really important\n",
    "1. Get familiar with data augmentation, but remember you can't precompute your convolutional layers\n",
    "1. Dropout is super important, but the value is dependent on your training set size, so you need to relearn it after you finish with the sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "t_batches = get_batches(data_path + 'train', batch_size=batch_size)\n",
    "v_batches = get_batches(data_path + 'valid', batch_size=2*batch_size, shuffle=False)\n",
    "\n",
    "# (\n",
    "#     val_classes, trn_classes, \n",
    "#     val_labels, trn_labels, \n",
    "#     val_filenames, filenames,\n",
    "#     test_filename\n",
    "# ) = get_classes(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lin_model():\n",
    "\n",
    "    # starting with BatchNormalization saves us from having to normalize our input manually\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 224, 224)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 150528)        0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            1505290     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,505,302\n",
      "Trainable params: 1,505,296\n",
      "Non-trainable params: 6\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1500/1500 [==============================] - 33s - loss: 13.1149 - acc: 0.1413 - val_loss: 13.9727 - val_acc: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef672de510>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = get_lin_model()\n",
    "lm.summary()\n",
    "\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.nb_sample, \n",
    "    nb_epoch=1, \n",
    "    validation_data=v_batches, \n",
    "    nb_val_samples=v_batches.nb_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(lm.predict_generator(t_batches, t_batches.nb_sample)[:10],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is entirely predicting 2 of the classes.  Not very useful.  Lower the learning rate and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 34s - loss: 2.4478 - acc: 0.1987 - val_loss: 3.9639 - val_acc: 0.1620\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 27s - loss: 1.8161 - acc: 0.3767 - val_loss: 2.4579 - val_acc: 0.2950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef64ee3310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = get_lin_model()\n",
    "\n",
    "lm.optimizer.lr.set_value(1e-5)\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.nb_sample, \n",
    "    nb_epoch=2, \n",
    "    validation_data=v_batches, \n",
    "    nb_val_samples=v_batches.nb_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 9.6578 - acc: 0.3080 - val_loss: 12.8233 - val_acc: 0.1690\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 9.6226 - acc: 0.3627 - val_loss: 9.6096 - val_acc: 0.3770\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 8.8492 - acc: 0.4240 - val_loss: 9.9640 - val_acc: 0.3540\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 8.8242 - acc: 0.4307 - val_loss: 9.1931 - val_acc: 0.4080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef64ee3d90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.optimizer.lr.set_value(1e-3)\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.nb_sample, \n",
    "    nb_epoch=4,\n",
    "    validation_data=v_batches, \n",
    "    nb_val_samples=v_batches.nb_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of 0.43 -- almost too good to be true, and definitely better than chance.\n",
    "\n",
    "Now lets make sure validation set is large enough to ensure a stable accuracy metric across runs, so we aren't making the wrong generalization about the results we're observing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.19,  0.41],\n",
       "       [ 9.26,  0.41],\n",
       "       [ 9.46,  0.39],\n",
       "       [ 9.08,  0.41],\n",
       "       [ 9.23,  0.41],\n",
       "       [ 9.24,  0.4 ],\n",
       "       [ 9.41,  0.4 ],\n",
       "       [ 9.04,  0.42],\n",
       "       [ 9.19,  0.41],\n",
       "       [ 8.98,  0.42]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_batches = get_batches(data_path+'valid', batch_size=2*batch_size)\n",
    "val_res = [lm.evaluate_generator(r_batches, r_batches.nb_sample) for i in range(10)]\n",
    "np.round(val_res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consisent results.  Accuracy increases larger than 2% can't be attributed to chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n",
      "Epoch 1/6\n",
      "1500/1500 [==============================] - 33s - loss: 2.3704 - acc: 0.2233 - val_loss: 5.2292 - val_acc: 0.1640\n",
      "Epoch 2/6\n",
      "1500/1500 [==============================] - 27s - loss: 1.7287 - acc: 0.4407 - val_loss: 3.2239 - val_acc: 0.2150\n",
      "Learning rate: 0.0001\n",
      "Epoch 3/6\n",
      "1500/1500 [==============================] - 26s - loss: 1.2193 - acc: 0.6033 - val_loss: 2.6735 - val_acc: 0.4290\n",
      "Epoch 4/6\n",
      "1500/1500 [==============================] - 25s - loss: 0.5838 - acc: 0.8267 - val_loss: 2.2709 - val_acc: 0.4160\n",
      "Epoch 5/6\n",
      "1500/1500 [==============================] - 26s - loss: 0.2921 - acc: 0.9127 - val_loss: 2.3365 - val_acc: 0.4500\n",
      "Epoch 6/6\n",
      "1500/1500 [==============================] - 26s - loss: 0.1682 - acc: 0.9627 - val_loss: 1.9739 - val_acc: 0.5060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5197c20ed0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = get_lin_model()\n",
    "\n",
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-4, 4)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(lm2, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# lrsched = keras.callbacks.LearningRateScheduler(\n",
    "#     fastai.utils.list_rate_schedule([\n",
    "#         (1e-5, 2),\n",
    "#         (1e-4, 4)\n",
    "#     ],\n",
    "#     output=True\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# lm2.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=6, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample,\n",
    "#     callbacks=[lrsched]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_20 (BatchNorm (None, 3, 224, 224)   12          batchnormalization_input_14[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)             (None, 150528)        0           batchnormalization_20[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 10)            1505290     flatten_14[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,505,302\n",
      "Trainable params: 1,505,296\n",
      "Non-trainable params: 6\n",
      "____________________________________________________________________________________________________\n",
      "Learning rate: 1e-05\n",
      "Epoch 1/6\n",
      "1500/1500 [==============================] - 33s - loss: 2.3988 - acc: 0.1860 - val_loss: 3.5731 - val_acc: 0.2420\n",
      "Epoch 2/6\n",
      "1500/1500 [==============================] - 25s - loss: 1.7132 - acc: 0.4400 - val_loss: 2.5621 - val_acc: 0.3670\n",
      "Learning rate: 0.0001\n",
      "Epoch 3/6\n",
      "1500/1500 [==============================] - 26s - loss: 1.2575 - acc: 0.5853 - val_loss: 2.5730 - val_acc: 0.4270\n",
      "Epoch 4/6\n",
      "1500/1500 [==============================] - 26s - loss: 0.5309 - acc: 0.8293 - val_loss: 1.7671 - val_acc: 0.4440\n",
      "Epoch 5/6\n",
      "1500/1500 [==============================] - 26s - loss: 0.3393 - acc: 0.8907 - val_loss: 1.4484 - val_acc: 0.5720\n",
      "Epoch 6/6\n",
      "1500/1500 [==============================] - 26s - loss: 0.1573 - acc: 0.9680 - val_loss: 1.5143 - val_acc: 0.5870\n",
      "Found 1000 images belonging to 10 classes.\n",
      "[[ 1.51  0.59]\n",
      " [ 1.56  0.59]\n",
      " [ 1.48  0.59]\n",
      " [ 1.46  0.61]\n",
      " [ 1.54  0.57]\n",
      " [ 1.52  0.59]\n",
      " [ 1.46  0.6 ]\n",
      " [ 1.52  0.58]\n",
      " [ 1.51  0.59]\n",
      " [ 1.47  0.59]]\n"
     ]
    }
   ],
   "source": [
    "%run ../scripts/state-farm/linear-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regularized linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reglin_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax', W_regularizer=l2(0.01))\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 35s - loss: 2.5796 - acc: 0.1900 - val_loss: 4.4703 - val_acc: 0.1670\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 25s - loss: 1.9313 - acc: 0.4240 - val_loss: 3.6262 - val_acc: 0.2560\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 33s - loss: 1.4682 - acc: 0.5953 - val_loss: 4.5419 - val_acc: 0.2700\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.7639 - acc: 0.8360 - val_loss: 2.7208 - val_acc: 0.3640\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.4586 - acc: 0.9320 - val_loss: 2.1631 - val_acc: 0.5090\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.3389 - acc: 0.9787 - val_loss: 2.1320 - val_acc: 0.4410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef5eb68350>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlm = get_reglin_model()\n",
    "\n",
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-4, 4)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(rlm, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# rlm.optimizer.lr.set_value(1e-5)\n",
    "# rlm.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "\n",
    "# rlm.optimizer.lr.set_value(1e-4)\n",
    "# rlm.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=4, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single linear model with regularization is getting 45% accuracy.  Dipped at the end which means it's continuing to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_5 (BatchNorma (None, 3, 224, 224)   12          batchnormalization_input_5[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 150528)        0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 100)           15052900    flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 100)           400         dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 10)            1010        batchnormalization_6[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 15,054,322\n",
      "Trainable params: 15,054,116\n",
      "Non-trainable params: 206\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fc = get_fc_model()\n",
    "fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 39s - loss: 2.0811 - acc: 0.3260 - val_loss: 4.4430 - val_acc: 0.2200\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 31s - loss: 1.1320 - acc: 0.6967 - val_loss: 3.5516 - val_acc: 0.2420\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 34s - loss: 1.9149 - acc: 0.3753 - val_loss: 11.7057 - val_acc: 0.0810\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 25s - loss: 1.3464 - acc: 0.5887 - val_loss: 8.8302 - val_acc: 0.1290\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 26s - loss: 0.9059 - acc: 0.7620 - val_loss: 5.9977 - val_acc: 0.1590\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 26s - loss: 0.5755 - acc: 0.8487 - val_loss: 4.2873 - val_acc: 0.2830\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 25s - loss: 0.3329 - acc: 0.9273 - val_loss: 3.5137 - val_acc: 0.3470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef5dc68290>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-2, 5)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(fc, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# fc.optimizer.lr.set_value(1e-5)\n",
    "# fc.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "\n",
    "# fc.optimizer.lr.set_value(0.01)\n",
    "# fc.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning to learn the training set, but failing at validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_model(t_batches=t_batches, v_batches=v_batches, train=True):\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    if not train:\n",
    "        return model\n",
    "    \n",
    "    rates = [\n",
    "        (1e-4, 2),\n",
    "        (1e-3, 4)\n",
    "    ]\n",
    "\n",
    "    fastai.utils.fit_generator(model, t_batches, rates, val_batches=v_batches)\n",
    "    \n",
    "#     model.optimizer.lr.set_value(1e-4)\n",
    "#     h = model.fit_generator(\n",
    "#         t_batches, \n",
    "#         t_batches.nb_sample, \n",
    "#         nb_epoch=2, \n",
    "#         validation_data=v_batches, \n",
    "#         nb_val_samples=v_batches.nb_sample\n",
    "#     )\n",
    "\n",
    "#     model.optimizer.lr.set_value(1e-3)\n",
    "#     h = model.fit_generator(\n",
    "#         t_batches, \n",
    "#         t_batches.nb_sample, \n",
    "#         nb_epoch=4, \n",
    "#         validation_data=v_batches, \n",
    "#         nb_val_samples=v_batches.nb_sample\n",
    "#     )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 38s - loss: 1.4453 - acc: 0.5767 - val_loss: 2.1388 - val_acc: 0.2220\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 28s - loss: 0.3380 - acc: 0.9327 - val_loss: 2.0896 - val_acc: 0.2450\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 35s - loss: 0.2581 - acc: 0.9393 - val_loss: 1.8649 - val_acc: 0.3250\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.0760 - acc: 0.9893 - val_loss: 2.1105 - val_acc: 0.2950\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 29s - loss: 0.0178 - acc: 0.9987 - val_loss: 2.4252 - val_acc: 0.2910\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.0051 - acc: 1.0000 - val_loss: 2.6336 - val_acc: 0.2280\n"
     ]
    }
   ],
   "source": [
    "cm = get_conv_model()\n",
    "# cm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very rapidly learning the training data and failing to generalize to the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.1154 - acc: 0.3207 - val_loss: 1.9274 - val_acc: 0.2960\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 27s - loss: 1.1553 - acc: 0.6333 - val_loss: 2.2009 - val_acc: 0.1880\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 38s - loss: 1.1049 - acc: 0.6620 - val_loss: 1.9074 - val_acc: 0.3850\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.6780 - acc: 0.7940 - val_loss: 2.0739 - val_acc: 0.3630\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.4214 - acc: 0.8820 - val_loss: 2.2804 - val_acc: 0.3310\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.3258 - acc: 0.8993 - val_loss: 2.7369 - val_acc: 0.2650\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 34s - loss: 1.9176 - acc: 0.3973 - val_loss: 2.2439 - val_acc: 0.2740\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 28s - loss: 0.7708 - acc: 0.7860 - val_loss: 1.9366 - val_acc: 0.2760\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 35s - loss: 0.7150 - acc: 0.7840 - val_loss: 1.9357 - val_acc: 0.3570\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.3415 - acc: 0.9013 - val_loss: 1.8168 - val_acc: 0.3980\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.1738 - acc: 0.9573 - val_loss: 2.3124 - val_acc: 0.2000\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 29s - loss: 0.1065 - acc: 0.9740 - val_loss: 2.8378 - val_acc: 0.1480\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 36s - loss: 1.7012 - acc: 0.4827 - val_loss: 1.9316 - val_acc: 0.2870\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 26s - loss: 0.4458 - acc: 0.8960 - val_loss: 2.0243 - val_acc: 0.2620\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 0.4238 - acc: 0.8813 - val_loss: 1.9964 - val_acc: 0.1850\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.1763 - acc: 0.9640 - val_loss: 1.7337 - val_acc: 0.4380\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 28s - loss: 0.0602 - acc: 0.9873 - val_loss: 1.7106 - val_acc: 0.4890\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.0288 - acc: 0.9953 - val_loss: 1.7242 - val_acc: 0.4750\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(shear_range=0.1)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 38s - loss: 2.1082 - acc: 0.3393 - val_loss: 2.7930 - val_acc: 0.1790\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 26s - loss: 0.9799 - acc: 0.6927 - val_loss: 2.0522 - val_acc: 0.2930\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 0.9304 - acc: 0.7147 - val_loss: 2.4012 - val_acc: 0.3160\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.5048 - acc: 0.8600 - val_loss: 1.9779 - val_acc: 0.3050\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.3288 - acc: 0.9100 - val_loss: 1.8143 - val_acc: 0.3710\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.2417 - acc: 0.9307 - val_loss: 1.6527 - val_acc: 0.3770\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 34s - loss: 1.6338 - acc: 0.5053 - val_loss: 1.9960 - val_acc: 0.2460\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 28s - loss: 0.3635 - acc: 0.9240 - val_loss: 1.7223 - val_acc: 0.3640\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 35s - loss: 0.2395 - acc: 0.9400 - val_loss: 1.7038 - val_acc: 0.4230\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.1028 - acc: 0.9807 - val_loss: 1.7499 - val_acc: 0.3690\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.0247 - acc: 0.9980 - val_loss: 1.8515 - val_acc: 0.4990\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.0074 - acc: 1.0000 - val_loss: 1.9942 - val_acc: 0.4740\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(channel_shift_range=20)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Learning rate: 0.0001\n",
      "Epoch 1/6\n",
      "1500/1500 [==============================] - 36s - loss: 2.5537 - acc: 0.2220 - val_loss: 2.2636 - val_acc: 0.2350\n",
      "Epoch 2/6\n",
      "1500/1500 [==============================] - 26s - loss: 1.8250 - acc: 0.3987 - val_loss: 2.1047 - val_acc: 0.2230\n",
      "Learning rate: 0.001\n",
      "Epoch 3/6\n",
      "1500/1500 [==============================] - 28s - loss: 1.8220 - acc: 0.4273 - val_loss: 2.4602 - val_acc: 0.2240\n",
      "Epoch 4/6\n",
      "1500/1500 [==============================] - 27s - loss: 1.3387 - acc: 0.5647 - val_loss: 2.0455 - val_acc: 0.2570\n",
      "Epoch 5/6\n",
      "1500/1500 [==============================] - 26s - loss: 1.1572 - acc: 0.6273 - val_loss: 1.8977 - val_acc: 0.2790\n",
      "Epoch 6/6\n",
      "1500/1500 [==============================] - 27s - loss: 1.0194 - acc: 0.6693 - val_loss: 1.7050 - val_acc: 0.3800\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    rotation_range=15,\n",
    "    channel_shift_range=20\n",
    ")\n",
    "batches = get_batches(data_path + 'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 35s - loss: 0.8708 - acc: 0.7173 - val_loss: 1.8102 - val_acc: 0.3260\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 30s - loss: 0.8201 - acc: 0.7467 - val_loss: 1.8645 - val_acc: 0.3180\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.7758 - acc: 0.7693 - val_loss: 1.8696 - val_acc: 0.3350\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.7207 - acc: 0.7720 - val_loss: 1.8789 - val_acc: 0.3320\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.6836 - acc: 0.7953 - val_loss: 1.8783 - val_acc: 0.3230\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.7266 - acc: 0.7640 - val_loss: 1.7379 - val_acc: 0.3880\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.6291 - acc: 0.8187 - val_loss: 1.7620 - val_acc: 0.3880\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 29s - loss: 0.6121 - acc: 0.8093 - val_loss: 1.6780 - val_acc: 0.4060\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.6106 - acc: 0.8140 - val_loss: 1.5946 - val_acc: 0.4750\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 29s - loss: 0.5899 - acc: 0.8273 - val_loss: 1.6256 - val_acc: 0.4640\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.5784 - acc: 0.8200 - val_loss: 1.4311 - val_acc: 0.5240\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.5918 - acc: 0.8240 - val_loss: 1.3713 - val_acc: 0.5470\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.5203 - acc: 0.8447 - val_loss: 1.3449 - val_acc: 0.5620\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.5410 - acc: 0.8460 - val_loss: 1.3906 - val_acc: 0.5830\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.4666 - acc: 0.8513 - val_loss: 1.2573 - val_acc: 0.5780\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.4907 - acc: 0.8513 - val_loss: 1.2590 - val_acc: 0.5980\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.4703 - acc: 0.8647 - val_loss: 1.3576 - val_acc: 0.5510\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.4774 - acc: 0.8667 - val_loss: 1.2087 - val_acc: 0.5750\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.4138 - acc: 0.8793 - val_loss: 1.1694 - val_acc: 0.5870\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.4326 - acc: 0.8707 - val_loss: 1.1758 - val_acc: 0.5870\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.4288 - acc: 0.8713 - val_loss: 1.0651 - val_acc: 0.6240\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 29s - loss: 0.3982 - acc: 0.8807 - val_loss: 1.1492 - val_acc: 0.6150\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.3987 - acc: 0.8807 - val_loss: 1.0347 - val_acc: 0.6660\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.3686 - acc: 0.8933 - val_loss: 0.9806 - val_acc: 0.6850\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.3594 - acc: 0.8993 - val_loss: 0.9700 - val_acc: 0.7000\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 28s - loss: 0.3462 - acc: 0.9040 - val_loss: 0.9481 - val_acc: 0.7070\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 32s - loss: 0.3325 - acc: 0.9093 - val_loss: 1.0252 - val_acc: 0.6960\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.3490 - acc: 0.8953 - val_loss: 0.9410 - val_acc: 0.7270\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 27s - loss: 0.3303 - acc: 0.9060 - val_loss: 1.0605 - val_acc: 0.7030\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 26s - loss: 0.3448 - acc: 0.8933 - val_loss: 0.9086 - val_acc: 0.7360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f518a1922d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates = [\n",
    "    (1e-4, 30),\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(model, batches, rates, val_batches=v_batches)\n",
    "\n",
    "# model.optimizer.lr.set_value(0.0001)\n",
    "# model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=25, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2109 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0532610309016597, 0.71692745405217939]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vf_batches = get_batches(full_data_path + 'valid', batch_size=2*batch_size, shuffle=False)\n",
    "model.evaluate_generator(vf_batches, vf_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_31 (BatchNorm (None, 3, 224, 224)   12          batchnormalization_input_12[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 32, 222, 222)  896         batchnormalization_31[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_32 (BatchNorm (None, 32, 222, 222)  128         convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_13 (MaxPooling2D)   (None, 32, 74, 74)    0           batchnormalization_32[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 64, 72, 72)    18496       maxpooling2d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_33 (BatchNorm (None, 64, 72, 72)    256         convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_14 (MaxPooling2D)   (None, 64, 24, 24)    0           batchnormalization_33[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 36864)         0           maxpooling2d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 200)           7373000     flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_34 (BatchNorm (None, 200)           800         dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 10)            2010        batchnormalization_34[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 7,395,598\n",
      "Trainable params: 7,395,000\n",
      "Non-trainable params: 598\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(data_path + 'state-farm-cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reload model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full training plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history = []\n",
    "\n",
    "# gen_t = image.ImageDataGenerator(\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.05,\n",
    "#     shear_range=0.1,\n",
    "#     rotation_range=15,\n",
    "#     channel_shift_range=20\n",
    "# )\n",
    "# batches = get_batches(data_path + 'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "# model = get_conv_model(train=False)\n",
    "# model.optimizer.lr.set_value(1e-4)\n",
    "# h = model.fit_generator(\n",
    "#     batches, \n",
    "#     batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "# history.append(h)\n",
    "\n",
    "# model.optimizer.lr.set_value(1e-3)\n",
    "# h = model.fit_generator(\n",
    "#     batches, \n",
    "#     batches.nb_sample, \n",
    "#     nb_epoch=4, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "# history.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.optimizer.lr.set_value(0.0001)\n",
    "# h = model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# history.append(h)\n",
    "# h = model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=25, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# history.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# acc = []\n",
    "# val_acc = []\n",
    "# for h in history:\n",
    "#     acc += h.history['acc']\n",
    "#     val_acc += h.history['val_acc']\n",
    "    \n",
    "# plt.plot(acc)\n",
    "# plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit\n",
    "\n",
    "Compute test set output and actually submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import kaggle\n",
    "\n",
    "def submission_df(preds, test_batches, classes):\n",
    "    # construct dataframe of the submission\n",
    "    index = pd.Series(\n",
    "        [f.split('/')[-1] for f in test_batches.filenames],\n",
    "        name='img'\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        preds,\n",
    "        index=index,\n",
    "        columns=classes\n",
    "    )\n",
    "\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n",
      "Found 20315 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(\n",
    "    full_data_path + 'test', shuffle=False, batch_size=batch_size * 2,\n",
    "    class_mode=None)\n",
    "train_batches = get_batches(\n",
    "    full_data_path + 'train', shuffle=False, batch_size=batch_size,\n",
    "    class_mode=None)\n",
    "classes = sorted(train_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(samp_data_path + 'state-farm-cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cba9304d7d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                                             \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                                             \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                                             pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# predict\n",
    "preds = model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = submission_df(preds, test_batches, classes)\n",
    "df = df.clip(0.05, 0.95)\n",
    "df.to_csv(full_data_path + 'submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    'kg',\n",
    "    'submit',\n",
    "    '-u', os.environ['KAGGLE_USERNAME'],\n",
    "    '-p', os.environ['KAGGLE_PASSWORD'],\n",
    "    '-c', 'state-farm-distracted-driver-detection',\n",
    "    full_data_path + 'submission.csv'\n",
    "]\n",
    "\n",
    "subprocess.call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
