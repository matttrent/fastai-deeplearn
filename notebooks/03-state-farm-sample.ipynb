{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: Tesla K80 (0000:00:1E.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import fastai\n",
    "import fastai.utils\n",
    "from fastai.fautils import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data_path = os.path.expanduser('~/data/state-farm/')\n",
    "samp_data_path = os.path.expanduser('~/data/sample-state-farm/')\n",
    "data_path = samp_data_path\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview\n",
    "\n",
    "This notebook shows a lot of exploratory work with the State Farm kaggle challenge.  The biggest takeaways are:\n",
    "\n",
    "1. Find the smallest sample size that produces consistent results\n",
    "1. Start with very small models and quickly work up in complexity, till you're overfitting\n",
    "1. Selecting the initial training rate, and adjusting it through training is really important\n",
    "1. Get familiar with data augmentation, but remember you can't precompute your convolutional layers\n",
    "1. Dropout is super important, but the value is dependent on your training set size, so you need to relearn it after you finish with the sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "t_batches = get_batches(data_path + 'train', batch_size=batch_size)\n",
    "v_batches = get_batches(data_path + 'valid', batch_size=2*batch_size, shuffle=False)\n",
    "\n",
    "(\n",
    "    val_classes, trn_classes, \n",
    "    val_labels, trn_labels, \n",
    "    val_filenames, filenames,\n",
    "    test_filename\n",
    ") = get_classes(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lin_model():\n",
    "\n",
    "    # starting with BatchNormalization saves us from having to normalize our input manually\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 3, 224, 224)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 150528)        0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            1505290     flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,505,302\n",
      "Trainable params: 1,505,296\n",
      "Non-trainable params: 6\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "1500/1500 [==============================] - 33s - loss: 13.1149 - acc: 0.1413 - val_loss: 13.9727 - val_acc: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef672de510>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = get_lin_model()\n",
    "lm.summary()\n",
    "\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.nb_sample, \n",
    "    nb_epoch=1, \n",
    "    validation_data=v_batches, \n",
    "    nb_val_samples=v_batches.nb_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(lm.predict_generator(t_batches, t_batches.nb_sample)[:10],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is entirely predicting 2 of the classes.  Not very useful.  Lower the learning rate and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 34s - loss: 2.4478 - acc: 0.1987 - val_loss: 3.9639 - val_acc: 0.1620\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 27s - loss: 1.8161 - acc: 0.3767 - val_loss: 2.4579 - val_acc: 0.2950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef64ee3310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = get_lin_model()\n",
    "\n",
    "lm.optimizer.lr.set_value(1e-5)\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.nb_sample, \n",
    "    nb_epoch=2, \n",
    "    validation_data=v_batches, \n",
    "    nb_val_samples=v_batches.nb_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 9.6578 - acc: 0.3080 - val_loss: 12.8233 - val_acc: 0.1690\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 9.6226 - acc: 0.3627 - val_loss: 9.6096 - val_acc: 0.3770\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 8.8492 - acc: 0.4240 - val_loss: 9.9640 - val_acc: 0.3540\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 8.8242 - acc: 0.4307 - val_loss: 9.1931 - val_acc: 0.4080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef64ee3d90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.optimizer.lr.set_value(1e-3)\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.nb_sample, \n",
    "    nb_epoch=4,\n",
    "    validation_data=v_batches, \n",
    "    nb_val_samples=v_batches.nb_sample\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of 0.43 -- almost too good to be true, and definitely better than chance.\n",
    "\n",
    "Now lets make sure validation set is large enough to ensure a stable accuracy metric across runs, so we aren't making the wrong generalization about the results we're observing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.19,  0.41],\n",
       "       [ 9.26,  0.41],\n",
       "       [ 9.46,  0.39],\n",
       "       [ 9.08,  0.41],\n",
       "       [ 9.23,  0.41],\n",
       "       [ 9.24,  0.4 ],\n",
       "       [ 9.41,  0.4 ],\n",
       "       [ 9.04,  0.42],\n",
       "       [ 9.19,  0.41],\n",
       "       [ 8.98,  0.42]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_batches = get_batches(data_path+'valid', batch_size=2*batch_size)\n",
    "val_res = [lm.evaluate_generator(r_batches, r_batches.nb_sample) for i in range(10)]\n",
    "np.round(val_res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consisent results.  Accuracy increases larger than 2% can't be attributed to chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n",
      "Epoch 1/6\n",
      "1500/1500 [==============================] - 33s - loss: 2.3704 - acc: 0.2233 - val_loss: 5.2292 - val_acc: 0.1640\n",
      "Epoch 2/6\n",
      "1500/1500 [==============================] - 27s - loss: 1.7287 - acc: 0.4407 - val_loss: 3.2239 - val_acc: 0.2150\n",
      "Learning rate: 0.0001\n",
      "Epoch 3/6\n",
      "1500/1500 [==============================] - 26s - loss: 1.2193 - acc: 0.6033 - val_loss: 2.6735 - val_acc: 0.4290\n",
      "Epoch 4/6\n",
      "1500/1500 [==============================] - 25s - loss: 0.5838 - acc: 0.8267 - val_loss: 2.2709 - val_acc: 0.4160\n",
      "Epoch 5/6\n",
      "1500/1500 [==============================] - 26s - loss: 0.2921 - acc: 0.9127 - val_loss: 2.3365 - val_acc: 0.4500\n",
      "Epoch 6/6\n",
      "1500/1500 [==============================] - 26s - loss: 0.1682 - acc: 0.9627 - val_loss: 1.9739 - val_acc: 0.5060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5197c20ed0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = get_lin_model()\n",
    "\n",
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-4, 4)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(lm2, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# lrsched = keras.callbacks.LearningRateScheduler(\n",
    "#     fastai.utils.list_rate_schedule([\n",
    "#         (1e-5, 2),\n",
    "#         (1e-4, 4)\n",
    "#     ],\n",
    "#     output=True\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# lm2.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=6, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample,\n",
    "#     callbacks=[lrsched]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %run ../fastai/state_farm.py reg-lin-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regularized linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reglin_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax', W_regularizer=l2(0.01))\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 35s - loss: 2.5796 - acc: 0.1900 - val_loss: 4.4703 - val_acc: 0.1670\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 25s - loss: 1.9313 - acc: 0.4240 - val_loss: 3.6262 - val_acc: 0.2560\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 33s - loss: 1.4682 - acc: 0.5953 - val_loss: 4.5419 - val_acc: 0.2700\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.7639 - acc: 0.8360 - val_loss: 2.7208 - val_acc: 0.3640\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.4586 - acc: 0.9320 - val_loss: 2.1631 - val_acc: 0.5090\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.3389 - acc: 0.9787 - val_loss: 2.1320 - val_acc: 0.4410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef5eb68350>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlm = get_reglin_model()\n",
    "\n",
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-4, 4)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(rlm, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# rlm.optimizer.lr.set_value(1e-5)\n",
    "# rlm.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "\n",
    "# rlm.optimizer.lr.set_value(1e-4)\n",
    "# rlm.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=4, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single linear model with regularization is getting 45% accuracy.  Dipped at the end which means it's continuing to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_5 (BatchNorma (None, 3, 224, 224)   12          batchnormalization_input_5[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 150528)        0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 100)           15052900    flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 100)           400         dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 10)            1010        batchnormalization_6[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 15,054,322\n",
      "Trainable params: 15,054,116\n",
      "Non-trainable params: 206\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fc = get_fc_model()\n",
    "fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 39s - loss: 2.0811 - acc: 0.3260 - val_loss: 4.4430 - val_acc: 0.2200\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 31s - loss: 1.1320 - acc: 0.6967 - val_loss: 3.5516 - val_acc: 0.2420\n",
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 34s - loss: 1.9149 - acc: 0.3753 - val_loss: 11.7057 - val_acc: 0.0810\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 25s - loss: 1.3464 - acc: 0.5887 - val_loss: 8.8302 - val_acc: 0.1290\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 26s - loss: 0.9059 - acc: 0.7620 - val_loss: 5.9977 - val_acc: 0.1590\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 26s - loss: 0.5755 - acc: 0.8487 - val_loss: 4.2873 - val_acc: 0.2830\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 25s - loss: 0.3329 - acc: 0.9273 - val_loss: 3.5137 - val_acc: 0.3470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef5dc68290>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-2, 5)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(fc, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# fc.optimizer.lr.set_value(1e-5)\n",
    "# fc.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "\n",
    "# fc.optimizer.lr.set_value(0.01)\n",
    "# fc.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning to learn the training set, but failing at validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_model(t_batches=t_batches, v_batches=v_batches, train=True):\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    if not train:\n",
    "        return model\n",
    "    \n",
    "    rates = [\n",
    "        (1e-4, 2),\n",
    "        (1e-3, 4)\n",
    "    ]\n",
    "\n",
    "    fastai.utils.fit_generator(model, t_batches, rates, val_batches=v_batches)\n",
    "    \n",
    "#     model.optimizer.lr.set_value(1e-4)\n",
    "#     h = model.fit_generator(\n",
    "#         t_batches, \n",
    "#         t_batches.nb_sample, \n",
    "#         nb_epoch=2, \n",
    "#         validation_data=v_batches, \n",
    "#         nb_val_samples=v_batches.nb_sample\n",
    "#     )\n",
    "\n",
    "#     model.optimizer.lr.set_value(1e-3)\n",
    "#     h = model.fit_generator(\n",
    "#         t_batches, \n",
    "#         t_batches.nb_sample, \n",
    "#         nb_epoch=4, \n",
    "#         validation_data=v_batches, \n",
    "#         nb_val_samples=v_batches.nb_sample\n",
    "#     )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 38s - loss: 1.4453 - acc: 0.5767 - val_loss: 2.1388 - val_acc: 0.2220\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 28s - loss: 0.3380 - acc: 0.9327 - val_loss: 2.0896 - val_acc: 0.2450\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 35s - loss: 0.2581 - acc: 0.9393 - val_loss: 1.8649 - val_acc: 0.3250\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.0760 - acc: 0.9893 - val_loss: 2.1105 - val_acc: 0.2950\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 29s - loss: 0.0178 - acc: 0.9987 - val_loss: 2.4252 - val_acc: 0.2910\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.0051 - acc: 1.0000 - val_loss: 2.6336 - val_acc: 0.2280\n"
     ]
    }
   ],
   "source": [
    "cm = get_conv_model()\n",
    "# cm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very rapidly learning the training data and failing to generalize to the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 33s - loss: 2.1154 - acc: 0.3207 - val_loss: 1.9274 - val_acc: 0.2960\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 27s - loss: 1.1553 - acc: 0.6333 - val_loss: 2.2009 - val_acc: 0.1880\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 38s - loss: 1.1049 - acc: 0.6620 - val_loss: 1.9074 - val_acc: 0.3850\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.6780 - acc: 0.7940 - val_loss: 2.0739 - val_acc: 0.3630\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.4214 - acc: 0.8820 - val_loss: 2.2804 - val_acc: 0.3310\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.3258 - acc: 0.8993 - val_loss: 2.7369 - val_acc: 0.2650\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 34s - loss: 1.9176 - acc: 0.3973 - val_loss: 2.2439 - val_acc: 0.2740\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 28s - loss: 0.7708 - acc: 0.7860 - val_loss: 1.9366 - val_acc: 0.2760\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 35s - loss: 0.7150 - acc: 0.7840 - val_loss: 1.9357 - val_acc: 0.3570\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.3415 - acc: 0.9013 - val_loss: 1.8168 - val_acc: 0.3980\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.1738 - acc: 0.9573 - val_loss: 2.3124 - val_acc: 0.2000\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 29s - loss: 0.1065 - acc: 0.9740 - val_loss: 2.8378 - val_acc: 0.1480\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 36s - loss: 1.7012 - acc: 0.4827 - val_loss: 1.9316 - val_acc: 0.2870\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 26s - loss: 0.4458 - acc: 0.8960 - val_loss: 2.0243 - val_acc: 0.2620\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 0.4238 - acc: 0.8813 - val_loss: 1.9964 - val_acc: 0.1850\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.1763 - acc: 0.9640 - val_loss: 1.7337 - val_acc: 0.4380\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 28s - loss: 0.0602 - acc: 0.9873 - val_loss: 1.7106 - val_acc: 0.4890\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 25s - loss: 0.0288 - acc: 0.9953 - val_loss: 1.7242 - val_acc: 0.4750\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(shear_range=0.1)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 38s - loss: 2.1082 - acc: 0.3393 - val_loss: 2.7930 - val_acc: 0.1790\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 26s - loss: 0.9799 - acc: 0.6927 - val_loss: 2.0522 - val_acc: 0.2930\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 34s - loss: 0.9304 - acc: 0.7147 - val_loss: 2.4012 - val_acc: 0.3160\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.5048 - acc: 0.8600 - val_loss: 1.9779 - val_acc: 0.3050\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 27s - loss: 0.3288 - acc: 0.9100 - val_loss: 1.8143 - val_acc: 0.3710\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.2417 - acc: 0.9307 - val_loss: 1.6527 - val_acc: 0.3770\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Epoch 1/2\n",
      "1500/1500 [==============================] - 34s - loss: 1.6338 - acc: 0.5053 - val_loss: 1.9960 - val_acc: 0.2460\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 28s - loss: 0.3635 - acc: 0.9240 - val_loss: 1.7223 - val_acc: 0.3640\n",
      "Epoch 1/4\n",
      "1500/1500 [==============================] - 35s - loss: 0.2395 - acc: 0.9400 - val_loss: 1.7038 - val_acc: 0.4230\n",
      "Epoch 2/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.1028 - acc: 0.9807 - val_loss: 1.7499 - val_acc: 0.3690\n",
      "Epoch 3/4\n",
      "1500/1500 [==============================] - 30s - loss: 0.0247 - acc: 0.9980 - val_loss: 1.8515 - val_acc: 0.4990\n",
      "Epoch 4/4\n",
      "1500/1500 [==============================] - 26s - loss: 0.0074 - acc: 1.0000 - val_loss: 1.9942 - val_acc: 0.4740\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(channel_shift_range=20)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-70521f31e085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_conv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f2604cbb9be7>\u001b[0m in \u001b[0;36mget_conv_model\u001b[0;34m(t_batches, v_batches, train)\u001b[0m\n\u001b[1;32m     28\u001b[0m     ]\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#     model.optimizer.lr.set_value(1e-4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/projects/fastai-deep-learning/fastai/utils.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, trn_batches, rates, val_batches, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtrn_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtrn_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m                                              \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m                                              \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                                              **self._function_kwargs)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid argument \"%s\" passed to K.function'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m                                         \u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                                         \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1793\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m             defaults)\n\u001b[0m\u001b[1;32m   1796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1659\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1661\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1662\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/gof/vm.pyc\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                                                  \u001b[0mno_recycling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                                                  impl=impl))\n\u001b[0m\u001b[1;32m   1048\u001b[0m                 \u001b[0mlinker_make_thunk_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthunk_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 935\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    936\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 839\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0mfill_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         cthunk, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1189\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                                     \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                                     \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                                     keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return (thunk,\n\u001b[1;32m   1133\u001b[0m                 [link.Container(input, storage) for input, storage in\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         ret = module.instantiate(error_storage,\n\u001b[0;32m-> 1603\u001b[0;31m                                  *(in_storage + out_storage + orphd))\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    rotation_range=15,\n",
    "    channel_shift_range=20\n",
    ")\n",
    "batches = get_batches(data_path + 'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = [\n",
    "    (1e-4, 30),\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(model, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# model.optimizer.lr.set_value(0.0001)\n",
    "# model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=25, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2109 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0532610309016597, 0.71692745405217939]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vf_batches = get_batches(full_data_path + 'valid', batch_size=2*batch_size, shuffle=False)\n",
    "model.evaluate_generator(vf_batches, vf_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_31 (BatchNorm (None, 3, 224, 224)   12          batchnormalization_input_12[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 32, 222, 222)  896         batchnormalization_31[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_32 (BatchNorm (None, 32, 222, 222)  128         convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_13 (MaxPooling2D)   (None, 32, 74, 74)    0           batchnormalization_32[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 64, 72, 72)    18496       maxpooling2d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_33 (BatchNorm (None, 64, 72, 72)    256         convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_14 (MaxPooling2D)   (None, 64, 24, 24)    0           batchnormalization_33[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 36864)         0           maxpooling2d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 200)           7373000     flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_34 (BatchNorm (None, 200)           800         dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 10)            2010        batchnormalization_34[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 7,395,598\n",
      "Trainable params: 7,395,000\n",
      "Non-trainable params: 598\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(data_path + 'state-farm-cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reload model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full training plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history = []\n",
    "\n",
    "# gen_t = image.ImageDataGenerator(\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.05,\n",
    "#     shear_range=0.1,\n",
    "#     rotation_range=15,\n",
    "#     channel_shift_range=20\n",
    "# )\n",
    "# batches = get_batches(data_path + 'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "# model = get_conv_model(train=False)\n",
    "# model.optimizer.lr.set_value(1e-4)\n",
    "# h = model.fit_generator(\n",
    "#     batches, \n",
    "#     batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "# history.append(h)\n",
    "\n",
    "# model.optimizer.lr.set_value(1e-3)\n",
    "# h = model.fit_generator(\n",
    "#     batches, \n",
    "#     batches.nb_sample, \n",
    "#     nb_epoch=4, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "# history.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.optimizer.lr.set_value(0.0001)\n",
    "# h = model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# history.append(h)\n",
    "# h = model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=25, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# history.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# acc = []\n",
    "# val_acc = []\n",
    "# for h in history:\n",
    "#     acc += h.history['acc']\n",
    "#     val_acc += h.history['val_acc']\n",
    "    \n",
    "# plt.plot(acc)\n",
    "# plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit\n",
    "\n",
    "Compute test set output and actually submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import kaggle\n",
    "\n",
    "def submission_df(preds, test_batches, classes):\n",
    "    # construct dataframe of the submission\n",
    "    index = pd.Series(\n",
    "        [f.split('/')[-1] for f in test_batches.filenames],\n",
    "        name='img'\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        preds,\n",
    "        index=index,\n",
    "        columns=classes\n",
    "    )\n",
    "\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n",
      "Found 20315 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = get_batches(\n",
    "    full_data_path + 'test', shuffle=False, batch_size=batch_size * 2,\n",
    "    class_mode=None)\n",
    "train_batches = get_batches(\n",
    "    full_data_path + 'train', shuffle=False, batch_size=batch_size,\n",
    "    class_mode=None)\n",
    "classes = sorted(train_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(samp_data_path + 'state-farm-cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cba9304d7d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                                             \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                                             \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                                             pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/envs/fastai/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# predict\n",
    "preds = model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = submission_df(preds, test_batches, classes)\n",
    "df = df.clip(0.05, 0.95)\n",
    "df.to_csv(full_data_path + 'submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    'kg',\n",
    "    'submit',\n",
    "    '-u', os.environ['KAGGLE_USERNAME'],\n",
    "    '-p', os.environ['KAGGLE_PASSWORD'],\n",
    "    '-c', 'state-farm-distracted-driver-detection',\n",
    "    full_data_path + 'submission.csv'\n",
    "]\n",
    "\n",
    "subprocess.call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
