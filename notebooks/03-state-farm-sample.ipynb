{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda/envs/deeplearn/lib/python3.6/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: Tesla K80 (0000:00:1E.0)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import fastai\n",
    "import fastai.utils\n",
    "from fastai.fautils import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data_path = os.path.expanduser('~/data/state-farm/')\n",
    "samp_data_path = os.path.expanduser('~/data/sample-state-farm/')\n",
    "data_path = samp_data_path\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview\n",
    "\n",
    "This notebook shows a lot of exploratory work with the State Farm kaggle challenge.  The biggest takeaways are:\n",
    "\n",
    "1. Find the smallest sample size that produces consistent results\n",
    "1. Start with very small models and quickly work up in complexity, till you're overfitting\n",
    "1. Selecting the initial training rate, and adjusting it through training is really important\n",
    "1. Get familiar with data augmentation, but remember you can't precompute your convolutional layers\n",
    "1. Dropout is super important, but the value is dependent on your training set size, so you need to relearn it after you finish with the sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "t_batches = get_batches(data_path + 'train', batch_size=batch_size)\n",
    "v_batches = get_batches(data_path + 'valid', batch_size=2*batch_size, shuffle=False)\n",
    "\n",
    "# (\n",
    "#     val_classes, trn_classes, \n",
    "#     val_labels, trn_labels, \n",
    "#     val_filenames, filenames,\n",
    "#     test_filename\n",
    "# ) = get_classes(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lin_model():\n",
    "\n",
    "    # starting with BatchNormalization saves us from having to normalize our input manually\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224, 224, 3)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_10 (Batc (None, 224, 224, 3)       896       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 150528)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1505290   \n",
      "=================================================================\n",
      "Total params: 1,506,186\n",
      "Trainable params: 1,505,738\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "23/23 [==============================] - 13s - loss: 13.0639 - acc: 0.1520 - val_loss: 14.9485 - val_acc: 0.0677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ba6f303c8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = get_lin_model()\n",
    "lm.summary()\n",
    "\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    steps_per_epoch=t_batches.samples//t_batches.batch_size,\n",
    "    validation_data=v_batches, \n",
    "    validation_steps=v_batches.samples//v_batches.batch_size,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ],\n",
       "       [ 0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.78,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.22],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(lm.predict_generator(t_batches, t_batches.samples//t_batches.batch_size)[:10],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is entirely predicting 2 of the classes.  Not very useful.  Lower the learning rate and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23/23 [==============================] - 12s - loss: 12.8395 - acc: 0.1598 - val_loss: 11.7134 - val_acc: 0.2672\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 9s - loss: 13.0014 - acc: 0.1836 - val_loss: 12.3460 - val_acc: 0.2271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ba67fb9b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = get_lin_model()\n",
    "\n",
    "lm.optimizer.lr.assign(1e-5)\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.samples//t_batches.batch_size,\n",
    "    validation_data=v_batches, \n",
    "    validation_steps=v_batches.samples//v_batches.batch_size,\n",
    "    epochs=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "23/23 [==============================] - 13s - loss: 12.5915 - acc: 0.2154 - val_loss: 11.8525 - val_acc: 0.2592\n",
      "Epoch 2/4\n",
      "23/23 [==============================] - 9s - loss: 12.5450 - acc: 0.2170 - val_loss: 12.0377 - val_acc: 0.2443\n",
      "Epoch 3/4\n",
      "23/23 [==============================] - 9s - loss: 12.6485 - acc: 0.2109 - val_loss: 13.3142 - val_acc: 0.1686\n",
      "Epoch 4/4\n",
      "23/23 [==============================] - 9s - loss: 12.7059 - acc: 0.2087 - val_loss: 13.9922 - val_acc: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ba61ec4e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.optimizer.lr.assign(1e-3)\n",
    "lm.fit_generator(\n",
    "    t_batches, \n",
    "    t_batches.samples//t_batches.batch_size,\n",
    "    validation_data=v_batches, \n",
    "    validation_steps=v_batches.samples//v_batches.batch_size,\n",
    "    epochs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of 0.43 -- almost too good to be true, and definitely better than chance.\n",
    "\n",
    "Now lets make sure validation set is large enough to ensure a stable accuracy metric across runs, so we aren't making the wrong generalization about the results we're observing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 14.19,   0.11],\n",
       "       [ 14.28,   0.11],\n",
       "       [ 14.35,   0.1 ],\n",
       "       [ 14.19,   0.11],\n",
       "       [ 14.31,   0.11],\n",
       "       [ 14.3 ,   0.11],\n",
       "       [ 14.19,   0.11],\n",
       "       [ 14.23,   0.11],\n",
       "       [ 14.24,   0.11],\n",
       "       [ 14.18,   0.11]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_batches = get_batches(data_path+'valid', batch_size=2*batch_size)\n",
    "val_res = [lm.evaluate_generator(r_batches, r_batches.samples//r_batches.batch_size) for i in range(10)]\n",
    "np.round(val_res, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consisent results.  Accuracy increases larger than 2% can't be attributed to chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 12s - loss: 2.4222 - acc: 0.1971 - val_loss: 3.6630 - val_acc: 0.2569\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 10s - loss: 1.6318 - acc: 0.4544 - val_loss: 2.5387 - val_acc: 0.3681\n",
      "Learning rate: 0.0001\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 9s - loss: 0.9634 - acc: 0.6992 - val_loss: 3.4708 - val_acc: 0.3589\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 9s - loss: 0.4049 - acc: 0.8772 - val_loss: 2.3735 - val_acc: 0.4931\n",
      "Epoch 5/6\n",
      "23/23 [==============================] - 10s - loss: 0.1778 - acc: 0.9483 - val_loss: 2.6717 - val_acc: 0.4381\n",
      "Epoch 6/6\n",
      "23/23 [==============================] - 9s - loss: 0.0992 - acc: 0.9769 - val_loss: 1.5524 - val_acc: 0.5550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ba601eb70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = get_lin_model()\n",
    "\n",
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-4, 4)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(lm2, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# lrsched = keras.callbacks.LearningRateScheduler(\n",
    "#     fastai.utils.list_rate_schedule([\n",
    "#         (1e-5, 2),\n",
    "#         (1e-4, 4)\n",
    "#     ],\n",
    "#     output=True\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# lm2.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=6, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample,\n",
    "#     callbacks=[lrsched]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_15 (Batc (None, 224, 224, 3)       896       \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 150528)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1505290   \n",
      "=================================================================\n",
      "Total params: 1,506,186\n",
      "Trainable params: 1,505,738\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Learning rate: 1e-05\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 13s - loss: 2.4071 - acc: 0.1936 - val_loss: 3.9977 - val_acc: 0.1987\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 9s - loss: 1.5928 - acc: 0.4863 - val_loss: 3.4246 - val_acc: 0.2232\n",
      "Learning rate: 0.0001\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 9s - loss: 1.1877 - acc: 0.6253 - val_loss: 3.4466 - val_acc: 0.3750\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 9s - loss: 0.5514 - acc: 0.8445 - val_loss: 3.5936 - val_acc: 0.3817\n",
      "Epoch 5/6\n",
      "23/23 [==============================] - 9s - loss: 0.2273 - acc: 0.9303 - val_loss: 2.3087 - val_acc: 0.5446\n",
      "Epoch 6/6\n",
      "23/23 [==============================] - 9s - loss: 0.0935 - acc: 0.9801 - val_loss: 1.8164 - val_acc: 0.5692\n",
      "Found 1000 images belonging to 10 classes.\n",
      "[[ 1.72  0.58]\n",
      " [ 1.74  0.57]\n",
      " [ 1.75  0.58]\n",
      " [ 1.76  0.57]\n",
      " [ 1.76  0.57]\n",
      " [ 1.73  0.58]\n",
      " [ 1.8   0.57]\n",
      " [ 1.8   0.56]\n",
      " [ 1.77  0.57]\n",
      " [ 1.75  0.57]]\n"
     ]
    }
   ],
   "source": [
    "%run ../scripts/state-farm/linear-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regularized linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reglin_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224, 224, 3)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax', kernel_regularizer=l2(0.01))\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 12s - loss: 2.4721 - acc: 0.2422 - val_loss: 4.2284 - val_acc: 0.1585\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 9s - loss: 1.7947 - acc: 0.4766 - val_loss: 3.1670 - val_acc: 0.2812\n",
      "Learning rate: 0.0001\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 9s - loss: 1.4279 - acc: 0.6163 - val_loss: 3.5642 - val_acc: 0.3772\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 9s - loss: 0.6723 - acc: 0.8442 - val_loss: 2.7119 - val_acc: 0.4844\n",
      "Epoch 5/6\n",
      "23/23 [==============================] - 10s - loss: 0.3985 - acc: 0.9427 - val_loss: 2.6314 - val_acc: 0.4833\n",
      "Epoch 6/6\n",
      "23/23 [==============================] - 9s - loss: 0.2844 - acc: 0.9856 - val_loss: 2.1230 - val_acc: 0.5636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b9f849eb8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlm = get_reglin_model()\n",
    "\n",
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-4, 4)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(rlm, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# rlm.optimizer.lr.set_value(1e-5)\n",
    "# rlm.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "\n",
    "# rlm.optimizer.lr.set_value(1e-4)\n",
    "# rlm.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=4, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single linear model with regularization is getting 45% accuracy.  Dipped at the end which means it's continuing to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fc_model():\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224, 224, 3)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_18 (Batc (None, 224, 224, 3)       896       \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 150528)            0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               15052900  \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 15,055,206\n",
      "Trainable params: 15,054,558\n",
      "Non-trainable params: 648\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fc = get_fc_model()\n",
    "fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n",
      "Epoch 1/7\n",
      "23/23 [==============================] - 12s - loss: 2.0311 - acc: 0.3476 - val_loss: 8.1658 - val_acc: 0.2411\n",
      "Epoch 2/7\n",
      "23/23 [==============================] - 9s - loss: 0.9194 - acc: 0.7656 - val_loss: 4.2537 - val_acc: 0.2790\n",
      "Learning rate: 0.01\n",
      "Epoch 3/7\n",
      "23/23 [==============================] - 9s - loss: 1.8750 - acc: 0.3930 - val_loss: 5.7433 - val_acc: 0.3337\n",
      "Epoch 4/7\n",
      "23/23 [==============================] - 9s - loss: 1.2124 - acc: 0.6472 - val_loss: 3.2536 - val_acc: 0.3583\n",
      "Epoch 5/7\n",
      "23/23 [==============================] - 9s - loss: 0.7847 - acc: 0.7954 - val_loss: 2.9960 - val_acc: 0.3917\n",
      "Epoch 6/7\n",
      "23/23 [==============================] - 9s - loss: 0.5156 - acc: 0.8748 - val_loss: 2.2975 - val_acc: 0.4833\n",
      "Epoch 7/7\n",
      "23/23 [==============================] - 9s - loss: 0.3396 - acc: 0.9258 - val_loss: 2.6493 - val_acc: 0.4386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b9e921208>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates = [\n",
    "    (1e-5, 2),\n",
    "    (1e-2, 5)\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(fc, t_batches, rates, val_batches=v_batches)\n",
    "\n",
    "# fc.optimizer.lr.set_value(1e-5)\n",
    "# fc.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "\n",
    "# fc.optimizer.lr.set_value(0.01)\n",
    "# fc.fit_generator(\n",
    "#     t_batches, \n",
    "#     t_batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning to learn the training set, but failing at validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_model(t_batches=t_batches, v_batches=v_batches, train=True):\n",
    "\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224, 224, 3)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    if not train:\n",
    "        return model\n",
    "    \n",
    "    rates = [\n",
    "        (1e-4, 2),\n",
    "        (1e-3, 4)\n",
    "    ]\n",
    "\n",
    "    fastai.utils.fit_generator(model, t_batches, rates, val_batches=v_batches)\n",
    "    \n",
    "#     model.optimizer.lr.set_value(1e-4)\n",
    "#     h = model.fit_generator(\n",
    "#         t_batches, \n",
    "#         t_batches.nb_sample, \n",
    "#         nb_epoch=2, \n",
    "#         validation_data=v_batches, \n",
    "#         nb_val_samples=v_batches.nb_sample\n",
    "#     )\n",
    "\n",
    "#     model.optimizer.lr.set_value(1e-3)\n",
    "#     h = model.fit_generator(\n",
    "#         t_batches, \n",
    "#         t_batches.nb_sample, \n",
    "#         nb_epoch=4, \n",
    "#         validation_data=v_batches, \n",
    "#         nb_val_samples=v_batches.nb_sample\n",
    "#     )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Epoch 1/6\n",
      "23/23 [==============================] - 56s - loss: 1.4494 - acc: 0.5647 - val_loss: 2.5730 - val_acc: 0.2695\n",
      "Epoch 2/6\n",
      "23/23 [==============================] - 55s - loss: 0.4277 - acc: 0.9072 - val_loss: 1.6465 - val_acc: 0.4495\n",
      "Learning rate: 0.001\n",
      "Epoch 3/6\n",
      "23/23 [==============================] - 55s - loss: 0.3793 - acc: 0.9070 - val_loss: 1.3735 - val_acc: 0.4197\n",
      "Epoch 4/6\n",
      "23/23 [==============================] - 55s - loss: 0.0840 - acc: 0.9891 - val_loss: 1.5877 - val_acc: 0.5011\n",
      "Epoch 5/6\n",
      " 8/23 [=========>....................] - ETA: 26s - loss: 0.0235 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "cm = get_conv_model()\n",
    "# cm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very rapidly learning the training data and failing to generalize to the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(shear_range=0.1)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(channel_shift_range=20)\n",
    "batches = get_batches(data_path+'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    rotation_range=15,\n",
    "    channel_shift_range=20\n",
    ")\n",
    "batches = get_batches(data_path + 'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "model = get_conv_model(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = [\n",
    "    (1e-4, 30),\n",
    "]\n",
    "\n",
    "fastai.utils.fit_generator(model, batches, rates, val_batches=v_batches)\n",
    "\n",
    "# model.optimizer.lr.set_value(0.0001)\n",
    "# model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=25, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_batches = get_batches(full_data_path + 'valid', batch_size=2*batch_size, shuffle=False)\n",
    "model.evaluate_generator(vf_batches, vf_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(data_path + 'state-farm-cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reload model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full training plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history = []\n",
    "\n",
    "# gen_t = image.ImageDataGenerator(\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.05,\n",
    "#     shear_range=0.1,\n",
    "#     rotation_range=15,\n",
    "#     channel_shift_range=20\n",
    "# )\n",
    "# batches = get_batches(data_path + 'train', gen_t, batch_size=batch_size)\n",
    "\n",
    "# model = get_conv_model(train=False)\n",
    "# model.optimizer.lr.set_value(1e-4)\n",
    "# h = model.fit_generator(\n",
    "#     batches, \n",
    "#     batches.nb_sample, \n",
    "#     nb_epoch=2, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "# history.append(h)\n",
    "\n",
    "# model.optimizer.lr.set_value(1e-3)\n",
    "# h = model.fit_generator(\n",
    "#     batches, \n",
    "#     batches.nb_sample, \n",
    "#     nb_epoch=4, \n",
    "#     validation_data=v_batches, \n",
    "#     nb_val_samples=v_batches.nb_sample\n",
    "# )\n",
    "# history.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.optimizer.lr.set_value(0.0001)\n",
    "# h = model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=5, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# history.append(h)\n",
    "# h = model.fit_generator(\n",
    "#     batches, batches.nb_sample, \n",
    "#     nb_epoch=25, \n",
    "#     validation_data=v_batches, nb_val_samples=v_batches.nb_sample)\n",
    "# history.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# acc = []\n",
    "# val_acc = []\n",
    "# for h in history:\n",
    "#     acc += h.history['acc']\n",
    "#     val_acc += h.history['val_acc']\n",
    "    \n",
    "# plt.plot(acc)\n",
    "# plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit\n",
    "\n",
    "Compute test set output and actually submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import kaggle\n",
    "\n",
    "def submission_df(preds, test_batches, classes):\n",
    "    # construct dataframe of the submission\n",
    "    index = pd.Series(\n",
    "        [f.split('/')[-1] for f in test_batches.filenames],\n",
    "        name='img'\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        preds,\n",
    "        index=index,\n",
    "        columns=classes\n",
    "    )\n",
    "\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = get_batches(\n",
    "    full_data_path + 'test', shuffle=False, batch_size=batch_size * 2,\n",
    "    class_mode=None)\n",
    "train_batches = get_batches(\n",
    "    full_data_path + 'train', shuffle=False, batch_size=batch_size,\n",
    "    class_mode=None)\n",
    "classes = sorted(train_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(samp_data_path + 'state-farm-cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "preds = model.predict_generator(test_batches, test_batches.samples // test_batches.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = submission_df(preds, test_batches, classes)\n",
    "df = df.clip(0.05, 0.95)\n",
    "df.to_csv(full_data_path + 'submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    'kg',\n",
    "    'submit',\n",
    "    '-u', os.environ['KAGGLE_USERNAME'],\n",
    "    '-p', os.environ['KAGGLE_PASSWORD'],\n",
    "    '-c', 'state-farm-distracted-driver-detection',\n",
    "    full_data_path + 'submission.csv'\n",
    "]\n",
    "\n",
    "subprocess.call(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
